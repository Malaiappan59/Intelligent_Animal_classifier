{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DR-eO17geWu"
   },
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMefrVPCg-60"
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "executionInfo": {
     "elapsed": 2136,
     "status": "ok",
     "timestamp": 1631133570451,
     "user": {
      "displayName": "Farhadh Manaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjzom9RFIhE87QnOAGHe8MxO5rL0lkrq7TJs5B1=s64",
      "userId": "02623529675324357487"
     },
     "user_tz": -330
    },
    "id": "sCV30xyVhFbE"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 1346,
     "status": "ok",
     "timestamp": 1631133589347,
     "user": {
      "displayName": "Farhadh Manaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjzom9RFIhE87QnOAGHe8MxO5rL0lkrq7TJs5B1=s64",
      "userId": "02623529675324357487"
     },
     "user_tz": -330
    },
    "id": "FIleuCAjoFD8",
    "outputId": "1186ff96-84ca-40bf-c35e-3ffb52527b24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.2'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxQxCBWyoGPE"
   },
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvE-heJNo3GG"
   },
   "source": [
    "### Preprocessing the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "executionInfo": {
     "elapsed": 443,
     "status": "error",
     "timestamp": 1627275858848,
     "user": {
      "displayName": "Mohamed Anishul Islam",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghe22Vms_iqLW3SjVdKaC5OUGRIjQ7sQ2jfWN8mL8Y=s64",
      "userId": "06867078216167203340"
     },
     "user_tz": -330
    },
    "id": "0koUcJMJpEBD",
    "outputId": "84e1955a-aff7-49b9-8d24-172e3ed498f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 974 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "training_set = train_datagen.flow_from_directory('/home/jetson/Desktop/classification/dataset/train_data',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 2,\n",
    "                                                 class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24173,
     "status": "ok",
     "timestamp": 1631133623919,
     "user": {
      "displayName": "Farhadh Manaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjzom9RFIhE87QnOAGHe8MxO5rL0lkrq7TJs5B1=s64",
      "userId": "02623529675324357487"
     },
     "user_tz": -330
    },
    "id": "_Fo3ZWm3MYJI",
    "outputId": "adcc931f-ed1b-424c-d3e6-7d97446f9f02"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "fiZlXD_1Kt6e",
    "outputId": "f6b8a03f-e411-4a5c-a07a-abc917b00ff5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': 0, 'fish': 1}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrCMmGw9pHys"
   },
   "source": [
    "### Preprocessing the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "SH4WzfOhpKc3",
    "outputId": "93d2a8f3-90c4-485e-c70f-e0093a4fb1a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 340 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "testing_set = test_datagen.flow_from_directory('/home/jetson/Desktop/classification/dataset/test_data',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 2,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af8O4l90gk7B"
   },
   "source": [
    "## Part 2 - Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ces1gXY2lmoX"
   },
   "source": [
    "### Initialising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "SAUt4UMPlhLS"
   },
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5YJj_XMl5LF"
   },
   "source": [
    "### Step 1 - Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "XPzPrMckl-hV"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tf87FpvxmNOJ"
   },
   "source": [
    "### Step 2 - Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "ncpqPl69mOac"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaTOgD8rm4mU"
   },
   "source": [
    "### Adding a second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "i_-FZjn_m8gk"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ciIvnewKt6k"
   },
   "source": [
    "# Adding a third convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "qaO97UJNKt6k"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmiEuvTunKfk"
   },
   "source": [
    "### Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "6AZeOGCvnNZn"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAoSECOm203v"
   },
   "source": [
    "### Step 4 - Full Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHZ7DZeLKt6m"
   },
   "source": [
    "# 1st dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "8GtmUlLd26Nq"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3R0f-8jKt6m"
   },
   "source": [
    "# 2nd dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "9lcbu0DDKt6o"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=64, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTldFvbX28Na"
   },
   "source": [
    "### Step 5 - Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "1p_Zj1Mc3Ko_"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6XkI90snSDl"
   },
   "source": [
    "## Part 3 - Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfrFQACEnc6i"
   },
   "source": [
    "### Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "NALksrNQpUlJ"
   },
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehS-v3MIpX2h"
   },
   "source": [
    "### Training the CNN on the Training set and evaluating it on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "XUj1W4PJptta",
    "outputId": "ccb18f0a-dbeb-466d-eb6b-b72ab543b151"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "487/487 [==============================] - 59s 121ms/step - loss: 0.2198 - accuracy: 0.9086 - val_loss: 0.6924 - val_accuracy: 0.7412\n",
      "Epoch 2/30\n",
      "487/487 [==============================] - 57s 116ms/step - loss: 0.2052 - accuracy: 0.9209 - val_loss: 0.6451 - val_accuracy: 0.7647\n",
      "Epoch 3/30\n",
      "487/487 [==============================] - 62s 126ms/step - loss: 0.2283 - accuracy: 0.9189 - val_loss: 0.6214 - val_accuracy: 0.7529\n",
      "Epoch 4/30\n",
      "487/487 [==============================] - 56s 114ms/step - loss: 0.1890 - accuracy: 0.9333 - val_loss: 0.6564 - val_accuracy: 0.7706\n",
      "Epoch 5/30\n",
      "487/487 [==============================] - 55s 113ms/step - loss: 0.1788 - accuracy: 0.9271 - val_loss: 0.7946 - val_accuracy: 0.7118\n",
      "Epoch 6/30\n",
      "487/487 [==============================] - 56s 115ms/step - loss: 0.1785 - accuracy: 0.9343 - val_loss: 1.0403 - val_accuracy: 0.7676\n",
      "Epoch 7/30\n",
      "487/487 [==============================] - 60s 122ms/step - loss: 0.2052 - accuracy: 0.9343 - val_loss: 0.7777 - val_accuracy: 0.7206\n",
      "Epoch 8/30\n",
      "487/487 [==============================] - 57s 116ms/step - loss: 0.1584 - accuracy: 0.9456 - val_loss: 0.9167 - val_accuracy: 0.7176\n",
      "Epoch 9/30\n",
      "487/487 [==============================] - 55s 112ms/step - loss: 0.1605 - accuracy: 0.9528 - val_loss: 0.6007 - val_accuracy: 0.7765\n",
      "Epoch 10/30\n",
      "487/487 [==============================] - 56s 114ms/step - loss: 0.1657 - accuracy: 0.9435 - val_loss: 0.9233 - val_accuracy: 0.7471\n",
      "Epoch 11/30\n",
      "487/487 [==============================] - 55s 112ms/step - loss: 0.1502 - accuracy: 0.9415 - val_loss: 0.8131 - val_accuracy: 0.7912\n",
      "Epoch 12/30\n",
      "487/487 [==============================] - 54s 110ms/step - loss: 0.1392 - accuracy: 0.9548 - val_loss: 0.7627 - val_accuracy: 0.7676\n",
      "Epoch 13/30\n",
      "487/487 [==============================] - 51s 105ms/step - loss: 0.1473 - accuracy: 0.9548 - val_loss: 0.7729 - val_accuracy: 0.7618\n",
      "Epoch 14/30\n",
      "487/487 [==============================] - 52s 104ms/step - loss: 0.1478 - accuracy: 0.9517 - val_loss: 1.0971 - val_accuracy: 0.7353\n",
      "Epoch 15/30\n",
      "487/487 [==============================] - 54s 110ms/step - loss: 0.1341 - accuracy: 0.9538 - val_loss: 1.0385 - val_accuracy: 0.7235\n",
      "Epoch 16/30\n",
      "487/487 [==============================] - 54s 110ms/step - loss: 0.1378 - accuracy: 0.9538 - val_loss: 0.8977 - val_accuracy: 0.7882\n",
      "Epoch 17/30\n",
      "487/487 [==============================] - 54s 111ms/step - loss: 0.1224 - accuracy: 0.9559 - val_loss: 1.1202 - val_accuracy: 0.7382\n",
      "Epoch 18/30\n",
      "487/487 [==============================] - 55s 113ms/step - loss: 0.1222 - accuracy: 0.9517 - val_loss: 0.9699 - val_accuracy: 0.7706\n",
      "Epoch 19/30\n",
      "487/487 [==============================] - 53s 108ms/step - loss: 0.1104 - accuracy: 0.9682 - val_loss: 1.0289 - val_accuracy: 0.7588\n",
      "Epoch 20/30\n",
      "487/487 [==============================] - 55s 112ms/step - loss: 0.1642 - accuracy: 0.9487 - val_loss: 0.9913 - val_accuracy: 0.7765\n",
      "Epoch 21/30\n",
      "487/487 [==============================] - 53s 109ms/step - loss: 0.1133 - accuracy: 0.9579 - val_loss: 1.0815 - val_accuracy: 0.7441\n",
      "Epoch 22/30\n",
      "487/487 [==============================] - 51s 105ms/step - loss: 0.1079 - accuracy: 0.9630 - val_loss: 1.0063 - val_accuracy: 0.7500\n",
      "Epoch 23/30\n",
      "487/487 [==============================] - 51s 104ms/step - loss: 0.1110 - accuracy: 0.9682 - val_loss: 0.9339 - val_accuracy: 0.7794\n",
      "Epoch 24/30\n",
      "487/487 [==============================] - 50s 103ms/step - loss: 0.0907 - accuracy: 0.9733 - val_loss: 1.4922 - val_accuracy: 0.7294\n",
      "Epoch 25/30\n",
      "487/487 [==============================] - 51s 104ms/step - loss: 0.0900 - accuracy: 0.9692 - val_loss: 1.3567 - val_accuracy: 0.7735\n",
      "Epoch 26/30\n",
      "487/487 [==============================] - 56s 115ms/step - loss: 0.0967 - accuracy: 0.9692 - val_loss: 1.1784 - val_accuracy: 0.7706\n",
      "Epoch 27/30\n",
      "487/487 [==============================] - 53s 110ms/step - loss: 0.1026 - accuracy: 0.9671 - val_loss: 1.0772 - val_accuracy: 0.7882\n",
      "Epoch 28/30\n",
      "487/487 [==============================] - 55s 112ms/step - loss: 0.1109 - accuracy: 0.9671 - val_loss: 0.9381 - val_accuracy: 0.7676\n",
      "Epoch 29/30\n",
      "487/487 [==============================] - 56s 116ms/step - loss: 0.0749 - accuracy: 0.9764 - val_loss: 1.0177 - val_accuracy: 0.7882\n",
      "Epoch 30/30\n",
      "487/487 [==============================] - 54s 109ms/step - loss: 0.0524 - accuracy: 0.9825 - val_loss: 1.2815 - val_accuracy: 0.7735\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7e2b854da0>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = training_set,validation_data = testing_set, epochs = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3PZasO0006Z"
   },
   "source": [
    "## Part 4 - Making a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "gsSiWEJY1BPB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('/home/jetson/Desktop/classification/dataset/train_data/cat/cat2.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image/255.0)\n",
    "training_set.class_indices\n",
    "if result[0][0] <= 0.5:\n",
    "  prediction = 'cat'\n",
    "else:\n",
    "  prediction = 'fish'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "ED9KB3I54c1i",
    "outputId": "fe9b7a96-5921-416c-f624-d23225a170c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nE69PFH9Kt6u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48_FlRFCKt6v"
   },
   "source": [
    "# saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "ekN-cGp0Kt6v",
    "outputId": "b24ba209-d70c-4adf-94f7-e4bd72434d0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn.cat_or_fish/assets\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "cnn.save(\"cnn.cat_or_fish\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrLJQ9r8Kt6w"
   },
   "source": [
    "# loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "executionInfo": {
     "elapsed": 419,
     "status": "error",
     "timestamp": 1631133653038,
     "user": {
      "displayName": "Farhadh Manaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjzom9RFIhE87QnOAGHe8MxO5rL0lkrq7TJs5B1=s64",
      "userId": "02623529675324357487"
     },
     "user_tz": -330
    },
    "id": "_kksS6b4Kt6w",
    "outputId": "b6dbffec-4dee-45dd-8081-2bdd5eebcd93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 29, 29, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 12, 12, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               147584    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 175,297\n",
      "Trainable params: 175,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from numpy import loadtxt\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "# load model\n",
    "cnn = load_model('/home/jetson/Desktop/classification/cnn.cat_or_fish')\n",
    "# summarize model.\n",
    "cnn.summary()\n",
    "\n",
    "# split into input and output variables\n",
    "\n",
    "# evaluate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "executionInfo": {
     "elapsed": 433,
     "status": "ok",
     "timestamp": 1631133659886,
     "user": {
      "displayName": "Farhadh Manaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjzom9RFIhE87QnOAGHe8MxO5rL0lkrq7TJs5B1=s64",
      "userId": "02623529675324357487"
     },
     "user_tz": -330
    },
    "id": "iYiHP2Z4Kt6x"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "executionInfo": {
     "elapsed": 420,
     "status": "error",
     "timestamp": 1631133665789,
     "user": {
      "displayName": "Farhadh Manaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjzom9RFIhE87QnOAGHe8MxO5rL0lkrq7TJs5B1=s64",
      "userId": "02623529675324357487"
     },
     "user_tz": -330
    },
    "id": "zoyRl2pyKt6x",
    "outputId": "743b2fc0-4324-44e5-f0df-3aa124d03fad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 974 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "training_set = train_datagen.flow_from_directory('/home/jetson/Desktop/classification/dataset/train_data',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 2,\n",
    "                                                 class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "executionInfo": {
     "elapsed": 449,
     "status": "error",
     "timestamp": 1631133675126,
     "user": {
      "displayName": "Farhadh Manaz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjzom9RFIhE87QnOAGHe8MxO5rL0lkrq7TJs5B1=s64",
      "userId": "02623529675324357487"
     },
     "user_tz": -330
    },
    "id": "uBg2Ri7XKt6z",
    "outputId": "3942440e-8676-4709-a94d-67855b8b5847"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 340 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "testing_set = test_datagen.flow_from_directory('/home/jetson/Desktop/classification/dataset/test_data',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 2,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "YD_x-CDjKt60"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "DPufCo5HKt61"
   },
   "outputs": [],
   "source": [
    "test_image = image.load_img('/home/jetson/Downloads/pc.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image/255.0)\n",
    "training_set.class_indices\n",
    "if result[0][0] <= 0.7:\n",
    "  prediction = 'cat'\n",
    "else:\n",
    "  prediction = 'fish'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "ZvHjR1ENKt61",
    "outputId": "2db9f763-7b1d-423f-c96a-de6a93663a68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "gu3TGeecKt63"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "fyIV6gJtKt65",
    "outputId": "59b6d45e-eaea-4998-f865-1159950dc979"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FKKwrfsKt66"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQSlr_reKt66"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "indian_vs_foreigner.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
